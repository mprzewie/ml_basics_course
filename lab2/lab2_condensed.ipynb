{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2.2 - Condensed Nearest Neighbors\n",
    "\n",
    "Celem zadania jest obserwacja wpływu kompresji Condensed Nearest Neighbours, w skrócie CNN, (najlepiej opisana w oryginalnej, bardzo krótkiej publikacji - patrz załącznik) na pracę klasyfikatora k-NN. Alternatywny (ale moim zdaniem miejscami niepełny) opis znajduje się także na Wikipedii (https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm#CNN_for_data_reduction) - tam też można znaleźć lepiej wyjaśniające problem obrazki.\n",
    "\n",
    "**Wykonanie rozwiązań: Marcin Przewięźlikowski**\n",
    "\n",
    "https://github.com/mprzewie/ml_basics_course"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from imblearn.under_sampling import CondensedNearestNeighbour\n",
    "from typing import Dict, Tuple\n",
    "import cv2\n",
    "from dataset_ops import (\n",
    "    load_dataset, visualize_dataset, slice_dataset, sliced_dataset_name\n",
    ")\n",
    "from copy import deepcopy\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Przygotowanie datasetów\n",
    "\n",
    "Korzystając np. z omawianej na zajęciach metody \"z paintem\" (lub innej), przygotuj 3 różne (ciekawe) dwuwymiarowe zbiory danych. Przynajmniej raz powinna wystąpić każda z poniższych sytuacji:\n",
    "- 3 lub więcej różnych klas\n",
    "- klasy dobrze odseparowane w danym regionie;\n",
    "- klasy częściowo przemieszane, nachodzące na siebie;\n",
    "- wyspa jednej klasy wewnątrz regionu drugiej;m\n",
    "- różne gęstości punktów wewnątrz poszczególnych klas;\n",
    "- nieregularne kształty obszaru jednej z klas;\n",
    "- niesymetryczny kształt całego zbioru (np. podłużne wrzeciono).\n",
    "\n",
    "Jeżeli korzystasz z metody \"z paintem\" to pamiętaj, by przed zapisaniem zbioru nałożyć na punkty niewielki szum. Przygotowane zbiory możesz wykorzystać w obu zadaniach domowych.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_datasets = {}\n",
    "n_datasets = 3\n",
    "x1_size, x2_size = (100, 100)\n",
    "for i in range(3):\n",
    "    X, y = load_dataset(f\"ds_{i + 1}.png\", space_size=(x1_size, x2_size), dropout=0.8)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "    orig_datasets[f\"{i}_train\"] = X_train, y_train\n",
    "    orig_datasets[f\"{i}_test\"] = X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, (X, y) in orig_datasets.items():\n",
    "    print(name)\n",
    "    visualize_dataset(X, y)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podobnie jak w poprzednim zadaniu z pary (Metric Learning) przygotowujemy zbiory danych, a następnie obserwujemy wygląd granicy decyzyjnej i % skuteczność klasyfikacji. Tym razem jednak korzystamy z następujących klasyfikatorów:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_coords = np.arange(0, 100)\n",
    "X_all = np.array([[[x0, x1] for x1 in all_coords] for x0 in all_coords]).reshape(-1, 2)\n",
    "y_all_ph = np.array([0 for _ in X_all])\n",
    "all_sliced = {}\n",
    "datasets = {}\n",
    "for n_slices in [1]:\n",
    "    for ds_name, (X, y) in orig_datasets.items():\n",
    "        sliced = slice_dataset(X, y, n_slices=n_slices)\n",
    "        for (x0, x1), (X_s, y_s) in sliced.items():\n",
    "            datasets[sliced_dataset_name(ds_name, n_slices, x0, x1)] = X_s, y_s\n",
    "    sliced_all_by_x0_x1 = slice_dataset(X_all, y_all_ph, n_slices=n_slices)\n",
    "    for (x0, x1), (X_s, y_s) in sliced_all_by_x0_x1.items():\n",
    "        all_sliced[(n_slices, x0, x1)] = X_s, y_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_knns(\n",
    "    knn_prototype: KNeighborsClassifier, \n",
    "    sampling=None\n",
    ") -> Dict[Tuple[int, int, int], KNeighborsClassifier]:\n",
    "    knn_dict = {}\n",
    "    for i in range(n_datasets):\n",
    "        ds_name = sliced_dataset_name(f\"{i}_train\", n_slices, 0,0)\n",
    "        X_train, y_train = datasets[ds_name]\n",
    "        if sampling:\n",
    "            X_train, y_train = sampling.fit_resample(X_train, y_train)\n",
    "        if X_train.shape[0] > 0:\n",
    "            knn = deepcopy(knn_prototype)\n",
    "            knn.fit(X_train, y_train)\n",
    "            knn_dict[(i, x0, x1)] = knn\n",
    "        else:\n",
    "            knn_dict[(i, x0, x1)] = None\n",
    "        plt.show()\n",
    "    return knn_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knns = OrderedDict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### zwykły k-NN z k=1 i metryką Euklidesa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KNN1_NAME = \"k1nn\"\n",
    "KNN1_PROTOTYPE = KNeighborsClassifier(n_neighbors=1, algorithm=\"brute\")\n",
    "knns[KNN1_NAME] = train_knns(KNN1_PROTOTYPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN z k=1 i metryką Euklidesa (losowo wybierający próbki w procedurze kondensacji);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KNN2_NAME = \"c1nn\"\n",
    "KNN2_PROTOTYPE = KNeighborsClassifier(n_neighbors=1, algorithm=\"brute\")\n",
    "knns[KNN2_NAME] = train_knns(\n",
    "    KNN2_PROTOTYPE,\n",
    "    sampling=CondensedNearestNeighbour(n_neighbors=1, sampling_strategy=\"all\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### zwykły k-NN z k=3 i metryką Euklidesa;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KNN3_NAME = \"k3nn\"\n",
    "KNN3_PROTOTYPE = KNeighborsClassifier(n_neighbors=3, algorithm=\"brute\")\n",
    "knns[KNN3_NAME] = train_knns(KNN3_PROTOTYPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN z k=3 i metryką Euklidesa (może być potrzebna niewielka adaptacja metody by pracowała z k>1 - zastanowić się jaka!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KNN4_NAME = \"c3nn\"\n",
    "KNN4_PROTOTYPE = KNeighborsClassifier(n_neighbors=3, algorithm=\"brute\")\n",
    "knns[KNN4_NAME] = train_knns(\n",
    "    KNN4_PROTOTYPE,\n",
    "    sampling=CondensedNearestNeighbour(n_neighbors=3, sampling_strategy=\"all\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(n_datasets):\n",
    "    _, y_train = orig_datasets[f\"{i}_train\"]\n",
    "    n_classes = len(np.unique(y_train))\n",
    "    print(\"dataset\", i, \"n_classes =\", n_classes)\n",
    "    for name, classifiers in knns.items():\n",
    "        print(\"\\t\",name)\n",
    "        accuracies = []\n",
    "        weights = []\n",
    "        n_slices = 1\n",
    "        knn = classifiers[(i, 0,0)]\n",
    "        ds_name = sliced_dataset_name(f\"{i}_test\", n_slices, x0, x1)\n",
    "        X_test, y_test = datasets[ds_name]\n",
    "        if knn is not None and X_test.shape[0] > 1:\n",
    "            weight = X_test.shape[0]\n",
    "            accuracy = knn.score(X_test, y_test)\n",
    "            X_all_s, _ = all_sliced[(n_slices, x0, x1)]\n",
    "            y_all = knn.predict(X_all_s)\n",
    "            visualize_dataset(X_all_s, y_all, n_classes)\n",
    "        else: \n",
    "            weight, accuracy = 0, 0\n",
    "        accuracies.append(accuracy)\n",
    "        weights.append(accuracy)\n",
    "        accuracies = np.array(accuracies)\n",
    "        weighs = np.array(weights)\n",
    "        total_acc = np.average(accuracies, weights=weights)\n",
    "        print(\"accuracy on\", i, \":\", total_acc)\n",
    "        plt.show()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Algorytmy CNN radzą sobie wyraźnie gorzej niż KNN, gdyż efektywnie są KNN'ami, które biorą do treningu mniej danych. Odbija się to na dokładności predykcji, a także jest widoczne w kształcie granic między klasami. \n",
    "\n",
    "* Dla podanych datasetów, C1NN radzi sobie lepiej niż C3NN"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
