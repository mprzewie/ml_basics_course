{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3.2 - Kernel trick\n",
    "\n",
    "**Wykonanie rozwiązań: Marcin Przewięźlikowski**\n",
    "\n",
    "https://github.com/mprzewie/ml_basics_course"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../lab2\")\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "from scipy.io import loadmat\n",
    "import matplotlib\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import os\n",
    "from dataset_ops import load_dataset, visualize_dataset\n",
    "from typing import Tuple\n",
    "from sklearn.decomposition import PCA\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hiperkula\n",
    "Wróćmy do hiperkuli wpisanej w hipersześcian z pierwszego zadania. Pokolorujmy  na czubkach narożników sześcianu na czerwono, punkty na krawędziach na żółto (mogą wymagać dodatkowej generacji, bo w losowym rozkładzie się raczej nie pojawią), punkty w jego wnętrzu (ale nie we wnętrzu kuli) na niebiesko, a punkty z kuli na zielono. Schemat kolorów przykładowy, każdy inny który pozwoli odróżnić elementy będzie ok. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def in_sphere(points: np.ndarray, R: float=1.0) -> np.ndarray:\n",
    "    n_dims = points.shape[1]\n",
    "    center = np.ones(n_dims) * R\n",
    "    return np.sqrt(((points - center) ** 2).sum(axis=1)) < R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cube_points(n_dims: int, n_points: int = 1000, R: float=1.0) -> np.ndarray:\n",
    "    return np.random.rand(n_points, n_dims) * 2 * R\n",
    "\n",
    "def vertex_points(n_dims: int, n_points: int = 100, R: float=1.0) -> np.ndarray:\n",
    "    points = np.random.rand(n_points, n_dims)\n",
    "    return (points > 1/2).astype(int) * 2 * R\n",
    "\n",
    "def edge_points(n_dims: int, n_points: int = 100, R: float=1.0) -> np.ndarray:\n",
    "    points = vertex_points(n_dims, n_points, R)\n",
    "    indices_to_randomize = np.random.randint(0, n_dims, n_points)\n",
    "    points[np.arange(n_points), indices_to_randomize] = np.random.rand(n_points) * 2 * R\n",
    "    return points\n",
    "\n",
    "def sphere_points(n_dims: int, n_points: int = 1000, R: float=1.0) -> np.ndarray:\n",
    "    points = cube_points(n_dims, n_points, R)\n",
    "    return points[in_sphere(points, R)]\n",
    "\n",
    "def non_sphere_points(n_dims: int, n_points: int = 1000, R: float=1.0) -> np.ndarray:\n",
    "    points = cube_points(n_dims, n_points, R)\n",
    "    return points[in_sphere(points, R) == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def points(\n",
    "    n_dims: int, \n",
    "    n_non_sphere: int=1000,\n",
    "    n_sphere: int = 1000,\n",
    "    n_edge: int = 100,\n",
    "    n_vertex: int = 100,\n",
    ") -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    non_sphere = non_sphere_points(n_dims, n_non_sphere)\n",
    "    sphere = sphere_points(n_dims, n_sphere)\n",
    "    edges = edge_points(n_dims, n_edge)\n",
    "    vertices = vertex_points(n_dims, n_vertex)\n",
    "    return non_sphere, sphere, edges, vertices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors_list = list(matplotlib.colors.get_named_colors_mapping().keys())\n",
    "np.random.seed(2)\n",
    "np.random.shuffle(colors_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_sphere, sphere, edges, vertices = points(2)\n",
    "\n",
    "for i, p in enumerate([non_sphere, sphere, edges, vertices]):\n",
    "    plt.scatter(p[:,0], p[:,1], c=colors_list[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Czyli:\n",
    "\n",
    "niebieskie -kula\n",
    "\n",
    "szare - wewnątrz sześcianu\n",
    "\n",
    "czerwone - krawędzie\n",
    "\n",
    "czarne - wierzchołki"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wykorzystać metodę PCA by wykonać wizualizację (rzut na płaszczyznę 2D) tejże sytuacji dla 3, 4, 5, 7 i 13 wymiarów. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dims = [3, 4, 5, 7, 10, 13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,15))\n",
    "fig.suptitle(f\"Rzut hipersześcianu na 2D\")\n",
    "for d, n_dims in enumerate(dims):\n",
    "    non_sphere, sphere, edges, vertices = points(n_dims)\n",
    "    all_points = np.concatenate([non_sphere, sphere, edges, vertices])\n",
    "    pca = PCA(n_components=2).fit(all_points)\n",
    "    plt.subplot(3,2, d+1, title=f\"dims = {n_dims}\")\n",
    "    for i, p in enumerate([non_sphere, sphere, edges, vertices]):\n",
    "        if p.shape[0] > 0:\n",
    "            p_c = pca.transform(p)\n",
    "            plt.scatter(p_c[:,0], p_c[:, 1], c=colors_list[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Powtórzyć to samo, ale dla wizualizacji 3D. Krótko opisać co widać. ;]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,25))\n",
    "fig.suptitle(f\"Rzut hipersześcianu na 3D\")\n",
    "for d, n_dims in enumerate(dims):\n",
    "    non_sphere, sphere, edges, vertices = points(n_dims)\n",
    "    all_points = np.concatenate([non_sphere, sphere, edges, vertices])\n",
    "    pca = PCA(n_components=3).fit(all_points)\n",
    "    ax = fig.add_subplot(3,2,d+1, projection='3d',  title=f\"dims = {n_dims}\")\n",
    "    for i, p in enumerate([non_sphere, sphere, edges, vertices]):\n",
    "        if p.shape[0] > 0:\n",
    "            p_c = pca.transform(p)\n",
    "            ax.scatter(xs=p_c[:,0], ys=p_c[:, 1], zs=p_c[:, 2], c=colors_list[i])\n",
    "    for spine in ax.spines.values():\n",
    "        spine.set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Komentarz\n",
    "\n",
    "Rzut hiperszescianów na płaszczyzny 2D i 3D wykonany przez PCA przypomina zwykłą wizualizację takiego hipersześcianu - poszczególne punkty znajdują się tam, gdzie można by się ich intuicyjnie spodziewać, widac to zwłaszcza na wizualizacjach sześcianu 3D.\n",
    "\n",
    "Jak wiemy z Klątwy Wymiarów, prawdopodobieństwo znalezienia w hiperszescianie punktów należących do hiperkuli maleje ze wzrostem wymiarowości, co również jest widoczne na wizulaizacjach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel PCA\n",
    "Wygenerujmy zbiór danych wyglądający +- tak, jak na rysunku w załączniku (punkty w różnych kolorach należą do różnych klas). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"ds_1.png\")\n",
    "X, y = ds\n",
    "X_uncentered = X\n",
    "# scentrowanie datasetu wokół (0,0)\n",
    "X = X - np.array([50,50])\n",
    "colors = ['k', 'b', 'r', 'c', 'm', 'y', 'k', 'w']\n",
    "for c in range(y.max() + 1):\n",
    "    X_c = X[y==c]\n",
    "    plt.scatter(X_c[:, 0], X_c[:, 1], color=colors[c], marker=\".\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Potraktujmy go klasycznym PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "X_t = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Przygotujmy diagram prezentujący rozkład punktów w nowo znalezionej przestrzeni."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in range(y.max() + 1):\n",
    "    X_c = X_t[y==c]\n",
    "    plt.scatter(X_c[:, 0], X_c[:, 1], color=colors[c], marker=\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Nanieśmy również na pierwotny rysunek strzałki prezentujące wektory będące znalezionymi \"principal components\" (gdzie długość wektora jest proporcjonalna do wariancji przez niego wyjaśnianej). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "origin = [[0], [0]]\n",
    "components = pca.components_ \n",
    "\n",
    "plt.quiver(\n",
    "    *origin, components[:, 0], components[:,1], \n",
    "    color=['r', 'g'], angles='xy', scale_units='xy', scale=0.03\n",
    ")\n",
    "for c in range(y.max() + 1):\n",
    "    X_c = X[y==c]\n",
    "    plt.scatter(X_c[:, 0], X_c[:, 1], color=colors[c], marker=\".\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spoiler - efekty będą dość trywialne (mapowanie z 2D w inne 2D). Można próbować uzyskać więcej stosując tzw. kernel trick - zamiast klasycznego iloczynu skalarnego wykorzysztać inną, odpowiednio spreparowaną funkcję. Zrzutujmy więc oba zbiory w nową przestrzeń, ale tym razem wykorzystując kernel PCA z kernelami:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cosine \n",
    "(tu warto sprawdzić jaki wpływ na wynik będzie miało wcześniejsze wyśrodkowanie danych lub jego brak) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15, 30))\n",
    "fig.suptitle(\"Cosine kernel PCA with dataset offset\")\n",
    "for i, offset in enumerate([0] + [2**i for i in range(9)]):\n",
    "    cosine_pca = KernelPCA(n_components=2, kernel=\"cosine\")\n",
    "    offset_arr = np.array([offset, offset])\n",
    "    X_offset = X + offset_arr\n",
    "    X_t = cosine_pca.fit_transform(X_offset)\n",
    "    origin = [[0], [0]]\n",
    "    components = cosine_pca.alphas_\n",
    "    plt.subplot(5,2, i+1)\n",
    "    plt.title(f\"offset = {offset_arr}\")\n",
    "    colors = ['k', 'b', 'r', 'c', 'm', 'y', 'k', 'w']\n",
    "    for c in range(y.max() + 1):\n",
    "        X_c = X_t[y==c]\n",
    "        plt.scatter(X_c[:, 0], X_c[:, 1], color=colors[c], marker=\".\")\n",
    "        plt.quiver(\n",
    "            *origin, components[y==c, 0], components[y==c,1], \n",
    "            color=colors[c],\n",
    "            angles='xy', \n",
    "            scale_units='xy', \n",
    "            scale=0.2\n",
    "        )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Komentarz\n",
    "\n",
    "Rozstawienie punktów w transformacji Cosine kernel PCA jest zależne od kąta, pod którym leżą te punkty od punktu $(0,0)$ w oryginalnym datasecie.\n",
    "\n",
    "Jeżeli wytrenujemy Cosine kernel PCA na zbiorze danych skoncentrowanym wokół $(0,0)$, transformacja tego zbioru będzie wyglądac jak okrag, natomiast ze zwiększaniem odległości \"centrum\" datasetu od $(0,0)$, większość punktów trafia na coraz mniejszy wycinek \"okręgu\", a tylko niektóre w inne jego miejsca."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### rbf \n",
    "(radial basis function - tu należy sprawdzić różne wartości parametru gamma wpływającego na pracę kernela, w tym także bardzo małe) oraz przygotujmy diagramy prezentujące efekty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15, 30))\n",
    "fig.suptitle(\"RBF kernel PCA\")\n",
    "for i, gamma in enumerate([2 ** i / (2**10) for i in range(10)]):\n",
    "    rbf_pca = KernelPCA(n_components=2, kernel=\"rbf\", gamma=gamma)\n",
    "    X_t = rbf_pca.fit_transform(X)\n",
    "    origin = [[0], [0]]\n",
    "    components = rbf_pca.alphas_\n",
    "    plt.subplot(5,2, i+1)\n",
    "    plt.title(f\"gamma = {gamma}\")\n",
    "    colors = ['k', 'b', 'r', 'c', 'm', 'y', 'k', 'w']\n",
    "    for c in range(y.max() + 1):\n",
    "        X_c = X_t[y==c]\n",
    "        plt.scatter(X_c[:, 0], X_c[:, 1], color=colors[c], marker=\".\")\n",
    "        plt.quiver(\n",
    "            *origin, components[y==c, 0], components[y==c,1], \n",
    "            color=colors[c],\n",
    "            angles='xy', \n",
    "            scale_units='xy', \n",
    "            scale=0.5\n",
    "        )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Komentarz\n",
    "\n",
    "RBF grupuje punkty o podobnej odległości od środka (średniej) zbioru danych. Wraz ze wzrostem parametru $\\gamma$, zmniejsza się rozrzut kierunków otrzymanych komponentów. Warto zauważyc, że jest to całkiem dobra transformacja naszego datasetu, gdyż w jego przypadku punkty tych samych klas leżą zazwyczaj w określonych odległościach od centrum datasetu."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
