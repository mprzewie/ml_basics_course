{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kaermorhenv import KaerMorhenv, map_from_csv, HyperParams, SARSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple, List, Optional, Callable\n",
    "from kaermohenv import KaerMorhenv, map_from_csv, HyperParams, SARSA\n",
    "from tqdm import tqdm\n",
    "from matplotlib import rc\n",
    "rc('animation', html='html5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = KaerMorhenv(\n",
    "    board=map_from_csv(\"map1.csv\"),\n",
    "    monster_coords=np.array([25, 25])\n",
    ")\n",
    "ax = env.render()\n",
    "plt.show(ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LearnFun = Callable[[np.ndarray, SARSA, HyperParams], np.ndarray]\n",
    "HParamGenerator = Callable[[int, int], HyperParams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learn(\n",
    "    q_table: np.ndarray,\n",
    "    sarsa: SARSA,\n",
    "    hparams: HyperParams\n",
    ") -> np.ndarray:\n",
    "    _, lr, dr = hparams\n",
    "    s1, a1, r, s2, _ = sarsa\n",
    "    q_table[s1, a1] += lr * (r + dr * q_table[s2].max() - q_table[s1, a1])\n",
    "    return q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(\n",
    "    env: KaerMorhenv,\n",
    "    q_table: np.ndarray,\n",
    "    state: int,\n",
    "    hparams: HyperParams,\n",
    ") -> int:\n",
    "    if np.random.rand() > hparams.exploration_rate:\n",
    "        action = np.argmax(q_table[state])\n",
    "    else:\n",
    "        action = env.action_space.sample()\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adventure(\n",
    "    env: KaerMorhenv,\n",
    "    q_table: np.ndarray,\n",
    "    hparams: HyperParams,\n",
    "    learn_fun: Optional[LearnFun] = None\n",
    "):\n",
    "    state = env.reset()\n",
    "    action = choose_action(env, q_table, state, hparams)\n",
    "    done = False\n",
    "    rewards = []\n",
    "    actions = [action]\n",
    "    while not done:\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        new_action = choose_action(env, q_table, new_state, hparams)\n",
    "        sarsa = SARSA(state, action, reward, new_state, new_action)\n",
    "        q_table = learn_fun(q_table, sarsa, hparams) if learn_fun else q_table\n",
    "        state = new_state\n",
    "        action = new_action\n",
    "        actions.append(action)\n",
    "        rewards.append(reward) \n",
    "    return q_table, sum(rewards), actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_train(\n",
    "    env: KaerMorhenv,\n",
    "    epochs: int,\n",
    "    learn_fun: LearnFun,\n",
    "    h_param_generator: HParamGenerator,\n",
    ") -> Tuple[np.ndarray, List[float]]:\n",
    "    q_table = np.zeros((env.nS, env.nA))\n",
    "    rewards_history = []\n",
    "    actions_history = []\n",
    "    for e in tqdm(range(epochs)):\n",
    "        h_params = h_param_generator(epochs, e)\n",
    "        q_table, reward, actions = adventure(env, q_table, h_params, learn_fun)\n",
    "        rewards_history.append(reward)\n",
    "        actions_history.append(actions)\n",
    "        \n",
    "    return q_table, rewards_history, actions_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_generator = lambda epochs, e: HyperParams(((epochs - e) /  epochs), 0.8, 0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 5000\n",
    "q_table, rewards_history, actions_history = q_train(\n",
    "    env, n_epochs, q_learn, param_generator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(n_epochs), rewards_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anim = env.render_actions(actions_history[-1], interval=200)\n",
    "anim.save(\"anim.gif\", writer=\"imagemagick\")\n",
    "anim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
