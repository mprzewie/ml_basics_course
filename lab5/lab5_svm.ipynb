{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 5 - Marginesy i kernele\n",
    "\n",
    "**Wykonanie rozwiązań: Marcin Przewięźlikowski**\n",
    "\n",
    "https://github.com/mprzewie/ml_basics_course"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../lab2\")\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset_ops import load_dataset\n",
    "from typing import Tuple, List\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from time import time\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pomiary klasyfikacji\n",
    "Wygenerujmy rozkład punktów podobny jak na powyższym obrazku. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_dataset(\"ds_1.png\", dropout=0.95)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all = []\n",
    "for x_0 in np.linspace(0, 100, 30):\n",
    "    for x_1 in np.linspace(0, 100, 30):\n",
    "        X_all.append([x_0, x_1])\n",
    "X_all = np.array(X_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in range(y.max() + 1):\n",
    "    X_c = X_train[y_train==c]\n",
    "    plt.scatter(X_c[:, 0], X_c[:, 1], c=(\"g\" if c==0 else \"k\")) \n",
    "plt.title(\"Training dataset\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in range(y.max() + 1):\n",
    "    X_c = X_test[y_test==c]\n",
    "    plt.scatter(X_c[:, 0], X_c[:, 1], c=(\"g\" if c==0 else \"k\"))\n",
    "plt.title(\"Test dataset\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dla takiego rozkładu zbadajmy jak będą się zachowywały w zależności od wybranego współczynnika C (rozumianego jako - współczynnik sterujący równowagą między zwiększaniem marginesu, a zmniejszaniem ilości punktów po złej stronie granicy) następujące wartości (do przetestowania rozsądny zakres i ilość współczynników C - tak aby pokazać trend na wykresie):\n",
    "* jaka jest szerokość marginesu;\n",
    "* jaki % punktów znalazł się po \"niewłaściwej\" stronie płaszczyzny dzielącej klasy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cs = 10 ** np.linspace(-5, 2, 15)\n",
    "# Cs = np.array([1])\n",
    "Cs_log = np.log10(Cs)\n",
    "Cs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C jest współczynnikiem, razy który w trakcie treningu będzie mnożony koszt wynikający z niedokładności klasyfikacji. Koszt wynikający z szerokości marginesu decyzyjnego można chyba interpretować jako regularyzację L2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_metrics(kernel: str, C: float, **kwargs) -> Tuple[float, float, float, float]:\n",
    "    svm = SVC(kernel=kernel, C=C, **kwargs)\n",
    "    t_start = time()\n",
    "    svm.fit(X_train, y_train)\n",
    "    t = time() - t_start\n",
    "    acc_train = svm.score(X_train, y_train)\n",
    "    acc_test = svm.score(X_test, y_test)    \n",
    "    scores = svm.decision_function(X_all)\n",
    "    scores_in_margin = np.abs(scores) < 1\n",
    "    # szerokość marginesu jest proporcjonalna do stosunku \n",
    "    # liczby punktów, które się w nim znalazły do liczby wszystkich punktów\n",
    "    # (coś na kształt metody monte carlo)\n",
    "    margin_width = scores_in_margin.sum() / len(X_all)\n",
    "    plt.show()\n",
    "    return acc_train, acc_test, margin_width, t, svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_metrics_measurements(kernel: str, **kwargs) -> Tuple[List[float], ...]:\n",
    "    accuracies = []\n",
    "    widths = []\n",
    "    times = []\n",
    "    svms = []\n",
    "    for C in Cs:\n",
    "        acc_train, acc_test, width, t, svm = svm_metrics(kernel, C, **kwargs)\n",
    "        accuracies.append((acc_train, acc_test))\n",
    "        widths.append(width)\n",
    "        times.append(t)\n",
    "        svms.append(svm)\n",
    "    return accuracies, widths, times, svms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_svm_metrics_measurements(**kwargs):\n",
    "    accuracies, widths, times, _ = svm_metrics_measurements(**kwargs)\n",
    "    kwargs_str = \", \".join([f\"{k} = {v}\" for (k,v) in kwargs.items()])\n",
    "    plt.figure(figsize=(6, 12))\n",
    "    plt.subplot(3, 1, 1)\n",
    "\n",
    "    plt.title(f\"Metrics of SVM with {kwargs_str} \\n\\n Accuracy\")\n",
    "    plt.xlabel(\"$log_{10}$(C)\")\n",
    "    plt.ylabel(\"accuracy\")\n",
    "    plt.plot(Cs_log, [a[0] for a in accuracies], label=\"train\")\n",
    "    plt.plot(Cs_log, [a[1] for a in accuracies], label=\"test\")\n",
    "    plt.legend()\n",
    "    plt.subplot(3, 1, 2)\n",
    "    plt.title(\"Margin width (percentage of all points with abs(decision) < 1)\")\n",
    "    plt.xlabel(\"$log_{10}$(C)\")\n",
    "    plt.ylabel(\"margin width\")\n",
    "    plt.plot(Cs_log, widths)\n",
    "    \n",
    "    plt.subplot(3, 1, 3)\n",
    "    plt.title(\"Fitting time\")\n",
    "    plt.xlabel(\"$log_{10}$(C)\")\n",
    "    plt.ylabel(\"time\")\n",
    "    plt.plot(Cs_log, times)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dokonajmy tych samych obliczeń dla:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zwykłego SVM, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_svm_metrics_measurements(kernel=\"linear\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Widać, że ze wzrostem $C$ maleje szerokość marginesu, co oznacza że im większe $C$, tym bardziej zależy nam na dokładności klasyfikacji, a mniej na wyraźnej granicy decyzyjnej.\n",
    "\n",
    "Ze wzrostem $C$ rośnie też czas treningu. Dokładność klasyfikacji zależy od większej liczby zmiennych (musimy dopasować się do przypadków treningowych) niż szerokości granicy decyzyjnej (dopasowujemy tylko jeden parametr). Ma zatem sens, że dopasowywanie dokładności klasyfikacji zajmie więcej iteracji."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM z kernelem wielomianowym stopnia trzeciego"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_svm_metrics_measurements(kernel=\"poly\",  gamma=\"scale\", coef0=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podobnie jak przy klasyfikacji, ze wzrostem $C$ maleje margines, ale także dokładność klasyfikacji zbioru testowego (podczas gdy rośnie dokładność na zbiorze treningowym). Mamy wtedy do czynienia z overfittingiem modelu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM z jakimś kernelem RBF (dla kilku wariantów współczynnika sterującego, jeżeli jest taki - miejmy pełną świadomość z jakiej RBF korzystamy!). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for gamma in 10 ** np.linspace(-5, 5, 5):\n",
    "    plot_svm_metrics_measurements(kernel=\"rbf\", gamma=gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVMy z kernelami RBF overfittują dla $C > 1$. Nie udaje im się też osiągnąć dużej pewności w predykcjach - prawie wszystkie punkty mają dokładność klasyfikacji w zakresie $(-1, 1)$.\n",
    "\n",
    "Nie widać też dużej korelacji między $C$ a czasem treningu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wizualizacja klasyfikacji\n",
    "Na koniec zwizualizujmy też efekty działania poprzez odpowiednie pomalowanie płaszczyzny (tak jak robiliśmy to przy metodzie k-NN). Z jednym wyjątkiem - tym razem niech odcień danego piksela zależy od odległości od płaszczyzny podziału (bliskie punkty = sporne = jasne, dalekie punkty = jednoznaczne = ciemne)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Opis kolorów:\n",
    "\n",
    "* czarny - punkty treningowe klasy $1$\n",
    "* zielony - punkty treningowe klasy $2$\n",
    "* czerwony - obszar zaklasyfikowany jako klasa $1$\n",
    "* żółty - obszar zaklasyfikowany jako klasa $2$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_svm(svm:SVC):\n",
    "    svm.fit(X_train, y_train)\n",
    "    decisions = svm.decision_function(X_all)\n",
    "    for x_s, d in zip(X_all, decisions):\n",
    "        a = min(np.abs(d), 1)\n",
    "        plt.scatter([x_s[0]], [x_s[1]], c=[\"r\" if d > 0 else \"y\"], alpha=a)\n",
    "    plt.scatter(X_train[:, 0], X_train[:, 1], \n",
    "                c=[(\"g\" if y==0 else \"k\") for y in y_train]\n",
    "               )\n",
    "    plt.title(f\"classification of {svm}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM z kernelami liniowymi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for svm in [\n",
    "    SVC(kernel=\"linear\", C=10e-4),\n",
    "    SVC(kernel=\"linear\", C=1),\n",
    "    SVC(kernel=\"linear\", C=100),\n",
    "\n",
    "]:\n",
    "    visualize_svm(svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dla danego datasetu SVMy liniowe mają podobne granice decyzyjne, niezależnie od parametru C."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM z kernelami wielomianowymi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for svm in [\n",
    "    SVC(kernel=\"poly\", coef0=0, C=10e-5, gamma=\"scale\"),\n",
    "    SVC(kernel=\"poly\", coef0=0, C=1, gamma=\"scale\"),\n",
    "\n",
    "    SVC(kernel=\"poly\", coef0=2, C=10e-5, gamma=\"scale\"),\n",
    "    SVC(kernel=\"poly\", coef0=2, C=1, gamma=\"scale\"),\n",
    "\n",
    "    SVC(kernel=\"poly\", coef0=4, C=10e-5, gamma=\"scale\"),\n",
    "    SVC(kernel=\"poly\", coef0=4, C=1, gamma=\"scale\"),\n",
    "    \n",
    "    SVC(kernel=\"poly\", coef0=8, C=10e-5, gamma=\"scale\"),\n",
    "    SVC(kernel=\"poly\", coef0=8, C=1, gamma=\"scale\"),\n",
    "]:\n",
    "    visualize_svm(svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Widać, że z małym $C$ (czyli dużą regularyzacją) SVM ma dość prostą granicę decyzyjną, niezależnie od stopnia wielomianu jaki dopasowuje. Zwiększając C, pozwalami modelowi nauczyć się granicy decyzyjnej bardziej biorącej pod uwagę punkty będące outlierami - a więc bardziej zoverfittować się do danych treningowych. Im większy stopień wielomianu, tym mniej \"niepewnych\" obszrów decyzyjnych."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM z kernelami RBF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for svm in [\n",
    "    SVC(kernel=\"rbf\", C=10e-5, gamma=10e-5),\n",
    "    SVC(kernel=\"rbf\", C=1, gamma=10e-5),\n",
    "    SVC(kernel=\"rbf\", C=100, gamma=10e-5),\n",
    "\n",
    "\n",
    "    SVC(kernel=\"rbf\", C=10e-5, gamma=10e-3),\n",
    "    SVC(kernel=\"rbf\", C=1, gamma=10e-3),\n",
    "    SVC(kernel=\"rbf\", C=100, gamma=10e-3),\n",
    "\n",
    "    \n",
    "    SVC(kernel=\"rbf\", C=10e-5, gamma=10e-2),\n",
    "    SVC(kernel=\"rbf\", C=1, gamma=10e-2),\n",
    "    SVC(kernel=\"rbf\", C=100, gamma=10e-2),\n",
    "\n",
    "]:\n",
    "    visualize_svm(svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dla takich samych wartości parametru $\\gamma$, wzrost parametru $C$ pozwala modelowi bardziej dopasować się do danych treningowych i ustalić obszary obecności danej klasy w oparciu o obecność punktów należących do niej.\n",
    "\n",
    "Im większy parametr $\\gamma$, tym te obszary wydają się być węższe i tym bardziej rośnie obszar niepewny."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
